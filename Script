import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import os
from scipy import stats
from scipy.spatial import distance
import re # Ensure this is imported at the top of your file, or inside the method
import sklearn
from matplotlib.backends.backend_pdf import PdfPages
import datetime

# Plot style
sns.set_theme(style="whitegrid")
plt.rcParams.update({"figure.autolayout": True})


class QualAnalyzerV20:
    def __init__(self, root):
        self.root = root
        self.root.title("Microcircuit Qual Analyzer V20 (Smart Limits + Inspector Slider Fix)")
        self.root.geometry("1600x950")

        # Data Storage
        self.data_dict = {}        # per sheet: wide data (SN + numeric cols)
        self.limits_dict = {}      # per sheet: {Parameter -> (LSL, USL)}
        self.param_map = {}        # base param -> [sheets]
        self.raw_previews = {}     # per sheet: raw DataFrame (no header)
        self.failures = []         # list of failure dicts
        self.current_file = None

        # Long-format tables
        self.measure_long = None   # DataFrame: Sheet, SN, Parameter, Value
        self.limit_long = None     # DataFrame: Sheet, Parameter, LSL, USL
        self.fail_long = None      # DataFrame: full failure table

        # Config Vars
        self.var_header_row = tk.IntVar(value=5)
        self.var_usl_row = tk.IntVar(value=12)
        self.var_lsl_row = tk.IntVar(value=13)
        self.var_data_start = tk.IntVar(value=18)
        self.var_data_end = tk.IntVar(value=0)
        self.var_sn_col = tk.IntVar(value=0)

        # Inspector sizing controls
        self.var_col_width = tk.IntVar(value=50)   # px
        self.var_row_height = tk.IntVar(value=22)   # px

        # Pagination
        self.page_size = 100
        self.current_page = 0
        self.total_rows = 0

        self.temps = ["Room", "Cold", "Hot"]
        self.is_tritemp = False

        # Matplotlib canvas references
        self.cpk_fig = None
        self.cpk_canvas = None
        self.trend_fig = None
        self.trend_canvas = None
        self.viz_fig = None
        self.viz_canvas = None

        # Distribution controls
        self.var_xmode = tk.StringVar(value="Sheet")
        self.var_show_limits = tk.StringVar(value="Off")

        # Treeview style for row height
        self.style = ttk.Style()
        self.style.configure("Inspector.Treeview", rowheight=self.var_row_height.get())
        self.style.configure("Treeview", rowheight=22)

        # --- Top controls ---
        control_frame = ttk.Frame(self.root, padding="10")
        control_frame.pack(fill=tk.X)

        btn_load = ttk.Button(control_frame, text="Load Data File (.xlsx)", command=self.load_file)
        btn_load.pack(side=tk.LEFT, padx=5)

        btn_reload = ttk.Button(control_frame, text="Reload Last File", command=self.reload_last_file)
        btn_reload.pack(side=tk.LEFT, padx=5)

        ttk.Label(control_frame, text="Page Size:").pack(side=tk.LEFT, padx=(20, 2))
        self.var_page_size = tk.IntVar(value=self.page_size)
        ttk.Entry(control_frame, textvariable=self.var_page_size, width=6).pack(side=tk.LEFT)

        ttk.Label(control_frame, text="Go to Page:").pack(side=tk.LEFT, padx=(20, 2))
        self.var_goto_page = tk.IntVar(value=1)
        ttk.Entry(control_frame, textvariable=self.var_goto_page, width=6).pack(side=tk.LEFT)
        ttk.Button(control_frame, text="Go", command=self.goto_page).pack(side=tk.LEFT, padx=2)

        self.lbl_status = ttk.Label(control_frame, text="Ready. Please load a file.")
        self.lbl_status.pack(side=tk.LEFT, padx=15)

        # Tabs
        self.tabs = ttk.Notebook(self.root)
        self.tabs.pack(expand=1, fill="both")

        self.tab_inspect = ttk.Frame(self.tabs)
        self.tab_cpk = ttk.Frame(self.tabs)
        self.tab_fail = ttk.Frame(self.tabs)
        self.tab_assess = ttk.Frame(self.tabs)
        self.tab_trend = ttk.Frame(self.tabs)
        self.tab_report = ttk.Frame(self.tabs)
        self.tab_visuals = ttk.Frame(self.tabs)
        self.tab_corr = ttk.Frame(self.tabs)  # <--- NEW TAB

        self.tabs.add(self.tab_inspect, text="Data Inspector")
        self.tabs.add(self.tab_cpk, text="Cpk Health Check")
        self.tabs.add(self.tab_fail, text="Failure Analysis")
        self.tabs.add(self.tab_assess, text="Lot Assessment")
        self.tabs.add(self.tab_trend, text="Detailed Trend")
        self.tabs.add(self.tab_corr, text="Correlation Matrix")  # <--- ADD TAB
        self.tabs.add(self.tab_report, text="Report Generator")
        self.tabs.add(self.tab_visuals, text="Distributions")

        self._setup_inspect_tab()
        self._setup_cpk_tab()
        self._setup_fail_tab()
        self._setup_assess_tab()
        self._setup_trend_tab()
        self._setup_corr_tab()  # <--- CALL SETUP
        self._setup_report_tab()
        self._setup_visuals_tab()

    # ----------------- Inspector tab -----------------
    def _setup_inspect_tab(self):
        frame = ttk.Frame(self.tab_inspect)
        frame.pack(fill=tk.BOTH, expand=True)

        config = ttk.LabelFrame(frame, text="Parsing Config", padding=10)
        config.pack(side=tk.LEFT, fill=tk.Y, padx=5, pady=5)

        def entry(lbl, var):
            f = ttk.Frame(config)
            f.pack(fill=tk.X, pady=2)
            ttk.Label(f, text=lbl, width=18).pack(side=tk.LEFT)
            ttk.Entry(f, textvariable=var, width=8).pack(side=tk.RIGHT)

        entry("Parameter (Row):", self.var_header_row)
        entry("Max Limit (Row):", self.var_usl_row)
        entry("Min Limit (Row):", self.var_lsl_row)
        entry("Data Start (Row):", self.var_data_start)
        entry("Data End (0=Auto):", self.var_data_end)
        entry("SN Column Idx:", self.var_sn_col)

        ttk.Button(config, text="Apply Changes", command=self.reparse_file).pack(fill=tk.X, pady=10)

        # Row-height slider only (column width slider is in top bar)
        size_box = ttk.LabelFrame(config, text="Inspector Size", padding=5)
        size_box.pack(fill=tk.X, pady=10)

        ttk.Label(size_box, text="Row Height").pack(anchor="w", pady=(0, 0))
        row_scale = ttk.Scale(
            size_box,
            from_=16,
            to=60,
            orient="horizontal",
            variable=self.var_row_height,
            command=lambda v: self._apply_inspector_row_height(),
        )
        row_scale.pack(fill=tk.X, pady=2)

        leg = ttk.LabelFrame(config, text="Legend", padding=5)
        leg.pack(fill=tk.X, pady=10)
        self._add_legend(leg, "Header Row", "#ffffcc")
        self._add_legend(leg, "Limits", "#ffcccc")
        self._add_legend(leg, "Data Start", "#ccffcc")
        self._add_legend(leg, "Data End", "#000000")

        right = ttk.Frame(frame)
        right.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)

        bar = ttk.Frame(right, padding=5)
        bar.pack(fill=tk.X)
        self.combo_inspect_sheet = ttk.Combobox(bar, state="readonly", width=20)
        self.combo_inspect_sheet.pack(side=tk.LEFT)
        self.combo_inspect_sheet.bind("<<ComboboxSelected>>", self.on_sheet_change)

        ttk.Button(bar, text="< Prev", command=self.prev_page).pack(side=tk.LEFT, padx=5)
        self.lbl_page = ttk.Label(bar, text="Page 1")
        self.lbl_page.pack(side=tk.LEFT)
        ttk.Button(bar, text="Next >", command=self.next_page).pack(side=tk.LEFT, padx=5)

        # Column width slider in top bar
        ttk.Label(bar, text="Col Width:").pack(side=tk.LEFT, padx=(15, 2))
        self.scale_col_width = tk.Scale(
            bar,
            from_=50,
            to=900,
            orient=tk.HORIZONTAL,
            showvalue=True,
            command=self.update_col_width
        )
        self.scale_col_width.set(self.var_col_width.get())
        self.scale_col_width.pack(side=tk.LEFT, padx=5)

        grid_f = ttk.Frame(right)
        grid_f.pack(fill=tk.BOTH, expand=True)

        self.tree_raw = ttk.Treeview(grid_f, show="headings", style="Inspector.Treeview")
        vsb = ttk.Scrollbar(grid_f, orient="vertical", command=self.tree_raw.yview)
        hsb = ttk.Scrollbar(grid_f, orient="horizontal", command=self.tree_raw.xview)
        self.tree_raw.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)

        self.tree_raw.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        hsb.pack(side=tk.BOTTOM, fill=tk.X)

        self.tree_raw.tag_configure("HEADER", background="#ffffcc")
        self.tree_raw.tag_configure("USL", background="#ffcccc")
        self.tree_raw.tag_configure("LSL", background="#ccccff")
        self.tree_raw.tag_configure("DATA_START", background="#ccffcc")
        self.tree_raw.tag_configure("DATA_END", background="black", foreground="white")

    def _add_legend(self, parent, text, color):
        f = ttk.Frame(parent)
        f.pack(anchor="w")
        tk.Label(f, bg=color, width=2).pack(side=tk.LEFT)
        ttk.Label(f, text=text).pack(side=tk.LEFT, padx=5)

    def update_col_width(self, val):
        """Real-time update of inspector column widths from slider."""
        try:
            w = int(float(val))
        except ValueError:
            return
        self.var_col_width.set(w)
        # Critical: stretch=False prevents auto-shrinking when widget resizes
        for col in self.tree_raw["columns"]:
            self.tree_raw.column(col, width=w, stretch=False, anchor="w")
        self.tree_raw.update_idletasks()

    def _apply_inspector_row_height(self):
        rh = int(self.var_row_height.get())
        self.style.configure("Inspector.Treeview", rowheight=rh)
        self.tree_raw.update_idletasks()

    # ----------------- Other tabs setup -----------------

    def _setup_corr_tab(self):
        frame = ttk.Frame(self.tab_corr)
        frame.pack(fill=tk.BOTH, expand=True)

        # 1. Control Bar
        ctrl = ttk.Frame(frame, padding=5)
        ctrl.pack(fill=tk.X)

        # Sheet Selection
        ttk.Label(ctrl, text="Sheet:").pack(side=tk.LEFT)
        self.combo_corr_sheet = ttk.Combobox(ctrl, state="readonly", width=15)
        self.combo_corr_sheet.pack(side=tk.LEFT, padx=5)

        # NEW: Temperature Filter Dropdown
        ttk.Label(ctrl, text="Temperature:").pack(side=tk.LEFT, padx=(15, 2))
        self.var_corr_temp = tk.StringVar(value="Room")
        self.combo_corr_temp = ttk.Combobox(ctrl, textvariable=self.var_corr_temp, state="readonly", width=10)
        self.combo_corr_temp["values"] = ("Room", "Cold", "Hot", "All")
        self.combo_corr_temp.pack(side=tk.LEFT, padx=5)
        self.combo_corr_temp.bind("<<ComboboxSelected>>", lambda e: self.plot_correlation_matrix())

        # --- NEW: Plot Type Selector (Heatmap vs Dendrogram) ---
        ttk.Label(ctrl, text="View:").pack(side=tk.LEFT, padx=(10, 2))
        self.var_corr_type = tk.StringVar(value="Heatmap")
        self.combo_corr_type = ttk.Combobox(ctrl, textvariable=self.var_corr_type, state="readonly", width=12)
        self.combo_corr_type["values"] = ("Heatmap", "Dendrogram (Clusters)")
        self.combo_corr_type.pack(side=tk.LEFT, padx=5)
        self.combo_corr_type.bind("<<ComboboxSelected>>", lambda e: self.plot_correlation_matrix())
        # -------------------------------------------------------

        # Action Button
        ttk.Button(ctrl, text="Update Matrix", command=self.plot_correlation_matrix).pack(side=tk.LEFT, padx=15)
        ttk.Label(ctrl, text="(Click cell for Scatter Plot)").pack(side=tk.LEFT, padx=5)
        self.lbl_corr_hint = ttk.Label(ctrl, text="(Click cell/leaf for details)", font=("Arial", 9, "italic"))
        self.lbl_corr_hint.pack(side=tk.LEFT, padx=5)

        # 2. Split View: Heatmap (Left) and Scatter Detail (Right)
        paned = ttk.PanedWindow(frame, orient=tk.HORIZONTAL)
        paned.pack(fill=tk.BOTH, expand=True)

        self.corr_heat_frame = ttk.Frame(paned)
        self.corr_scatter_frame = ttk.Frame(paned)

        paned.add(self.corr_heat_frame, weight=3)
        paned.add(self.corr_scatter_frame, weight=2)

        # Initialize Canvas placeholders
        self.corr_heat_canvas = None
        self.corr_scatter_canvas = None
        self.corr_fig_heat = None
        self.corr_fig_scatter = None

        # State storage
        self.current_corr_df = None
        self.current_corr_raw_data = None
        self.heatmap_ax = None

    def plot_correlation_matrix(self):
        """Generates Heatmap or Dendrogram based on selection."""
        import scipy.cluster.hierarchy as sch

        try:
            sheet = self.combo_corr_sheet.get()
            temp_mode = self.var_corr_temp.get()
            plot_type = self.var_corr_type.get()

            if not sheet or sheet not in self.data_dict: return

            # 1. Data Prep (Same as before)
            df = self.data_dict[sheet].copy()
            if "SN" in df.columns: df = df.set_index("SN")
            raw_numeric = df.select_dtypes(include=[np.number])

            # Filter by Temp
            if temp_mode != "All":
                cols = [c for c in raw_numeric.columns if temp_mode.lower() in c.lower()]
                if not cols:
                    messagebox.showinfo("Info", f"No '{temp_mode}' data found.")
                    return
                numeric_df = raw_numeric[cols]
            else:
                numeric_df = raw_numeric

            # Remove constants
            numeric_df = numeric_df.loc[:, numeric_df.std() > 1e-9]
            if numeric_df.shape[1] < 2: return

            # 2. Calculations
            self.current_corr_df = numeric_df.corr(method='pearson')
            self.current_corr_raw_data = numeric_df

            # 3. Setup Canvas
            if self.corr_heat_canvas is None:
                self.corr_fig_heat = plt.Figure(figsize=(8, 8), dpi=100)
                self.corr_heat_canvas = FigureCanvasTkAgg(self.corr_fig_heat, master=self.corr_heat_frame)
                self.corr_heat_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
                self.corr_heat_canvas.mpl_connect('button_press_event', self.on_heatmap_click)
            else:
                self.corr_fig_heat.clf()

            # 4. Draw Based on Type
            ax = self.corr_fig_heat.add_subplot(111)

            if plot_type == "Heatmap":
                # ... Existing Heatmap Logic ...
                n_params = len(numeric_df.columns)
                annot = True if n_params < 20 else False
                sns.heatmap(self.current_corr_df, ax=ax, cmap="RdBu_r", center=0,
                            square=True, annot=annot, fmt=".2f", annot_kws={"size": 8},
                            cbar_kws={'label': 'Pearson r'})
                ax.set_title(f"Correlation Matrix: {sheet} ({temp_mode})", fontweight="bold")
                plt.setp(ax.get_xticklabels(), rotation=45, ha="right")

                # Ensure the axis is stored for click events
                self.heatmap_ax = ax

            elif plot_type == "Dendrogram (Clusters)":
                # --- NEW: Dendrogram Logic ---
                # We use the correlation matrix to define 'distance' (1 - r)
                # Parameters that correlate highly (r=1.0) have distance 0.
                dist_matrix = 1 - self.current_corr_df.abs()

                # Perform Ward Clustering
                linkage = sch.linkage(sch.distance.squareform(dist_matrix), method='ward')

                # Plot
                dendro = sch.dendrogram(
                    linkage,
                    labels=numeric_df.columns,
                    ax=ax,
                    leaf_rotation=90,
                    leaf_font_size=8
                )

                ax.set_title(f"Parameter Clustering: {sheet}\n(Grouped by Behavior Similarity)", fontweight="bold")
                ax.set_ylabel("Cluster Distance (Height = Dissimilarity)")
                ax.grid(True, axis='y', alpha=0.3)

                # Disable the heatmap click handler for this view (or adapt it later)
                self.heatmap_ax = None
                self.lbl_corr_hint.config(text="(Clustering View: Height shows how distinct groups are)")

            self.corr_fig_heat.tight_layout()
            self.corr_heat_canvas.draw()

        except Exception as e:
            messagebox.showerror("Error", f"Plot failed:\n{str(e)}")

    def on_heatmap_click(self, event):
        """Robust click handler for the Heatmap."""
        try:
            if self.current_corr_df is None: return
            if event.xdata is None or event.ydata is None: return

            # Verify click is inside the heatmap axes
            if hasattr(self, 'heatmap_ax') and event.inaxes != self.heatmap_ax:
                return

            col_idx = int(event.xdata)
            row_idx = int(event.ydata)

            cols = self.current_corr_df.columns

            if 0 <= col_idx < len(cols) and 0 <= row_idx < len(cols):
                param_x = cols[col_idx]
                param_y = cols[row_idx]
                self.plot_corr_scatter(param_x, param_y)

        except Exception as e:
            print(f"Click Error: {e}")

    def plot_corr_scatter(self, px, py):
        """Draw the detailed regression plot."""
        try:
            if self.corr_scatter_canvas is None:
                self.corr_fig_scatter = plt.Figure(figsize=(5, 5), dpi=100)
                self.corr_scatter_canvas = FigureCanvasTkAgg(self.corr_fig_scatter, master=self.corr_scatter_frame)
                self.corr_scatter_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            else:
                self.corr_fig_scatter.clf()

            ax = self.corr_fig_scatter.add_subplot(111)

            data = self.current_corr_raw_data[[px, py]].dropna()
            r_val = self.current_corr_df.loc[px, py]

            sns.regplot(
                data=data,
                x=px,
                y=py,
                ax=ax,
                scatter_kws={'alpha': 0.5, 's': 30, 'edgecolor': 'black'},
                line_kws={'color': 'red'}
            )

            ax.set_title(f"R = {r_val:.4f}", fontweight="bold")
            # Truncate labels if too long
            ax.set_xlabel(px if len(px) < 20 else px[:17] + "...")
            ax.set_ylabel(py if len(py) < 20 else py[:17] + "...")
            ax.grid(True, alpha=0.3)

            ax.text(0.05, 0.95, f"N = {len(data)}", transform=ax.transAxes,
                    bbox=dict(facecolor='white', alpha=0.8))

            self.corr_fig_scatter.tight_layout()
            self.corr_scatter_canvas.draw()
        except Exception as e:
            messagebox.showerror("Plot Error", f"Could not plot scatter:\n{str(e)}")

    def _setup_cpk_tab(self):
        frame = ttk.Frame(self.tab_cpk)
        frame.pack(fill=tk.BOTH, expand=True)

        # --- NEW: Control Bar for Interactivity ---
        ctrl_frame = ttk.Frame(frame, padding=5)
        ctrl_frame.pack(fill=tk.X)

        ttk.Label(ctrl_frame, text="Visualization Mode:").pack(side=tk.LEFT)

        self.var_cpk_viz_mode = tk.StringVar(value="Histogram (Distribution)")
        self.combo_cpk_mode = ttk.Combobox(ctrl_frame, textvariable=self.var_cpk_viz_mode, state="readonly", width=35)
        self.combo_cpk_mode['values'] = (
            "Histogram (Distribution)",
            "Box Plot (Compare Test Steps)",
            "Bubble Plot (Identify Outliers)",
            "Centering Analysis (Precision vs Accuracy)"
        )
        self.combo_cpk_mode.pack(side=tk.LEFT, padx=5)
        self.combo_cpk_mode.bind("<<ComboboxSelected>>", lambda e: self.plot_cpk_viz())
        # ------------------------------------------

        paned = ttk.PanedWindow(frame, orient=tk.HORIZONTAL)
        paned.pack(fill=tk.BOTH, expand=True)

        self.cpk_plot_frame = ttk.Frame(paned)
        paned.add(self.cpk_plot_frame, weight=3)

        tree_frame = ttk.Frame(paned)
        self.tree_cpk = ttk.Treeview(tree_frame, columns=("Param", "Cpk", "Mean"), show="headings")
        for c in ("Param", "Cpk", "Mean"):
            self.tree_cpk.heading(c, text=c, command=lambda col=c: self._sort_cpk_tree(col, False))
            self.tree_cpk.column(c, anchor="w", width=120)
        sb = ttk.Scrollbar(tree_frame, orient="vertical", command=self.tree_cpk.yview)
        self.tree_cpk.configure(yscrollcommand=sb.set)
        self.tree_cpk.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        paned.add(tree_frame, weight=1)

    def _setup_fail_tab(self):
        frame = ttk.Frame(self.tab_fail)
        frame.pack(fill=tk.BOTH, expand=True)
        top = ttk.Frame(frame)
        top.pack(fill=tk.X)
        self.lbl_fail_summary = ttk.Label(frame, text="Total Failures: 0", font=("Arial", 12, "bold"))
        self.lbl_fail_summary.pack(in_=top, side=tk.LEFT, pady=10, padx=5)
        ttk.Button(top, text="Export Failures to CSV", command=self.export_failures_csv).pack(side=tk.LEFT, padx=5)

        cols = ("Sheet", "SN", "Parameter", "Value", "LSL", "USL", "Type")
        self.tree_fail = ttk.Treeview(frame, columns=cols, show="headings")
        for c in cols:
            self.tree_fail.heading(c, text=c)
        vsb = ttk.Scrollbar(frame, orient="vertical", command=self.tree_fail.yview)
        self.tree_fail.configure(yscrollcommand=vsb.set)
        self.tree_fail.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)

    def _setup_assess_tab(self):
        frame = ttk.Frame(self.tab_assess)
        frame.pack(fill=tk.BOTH, expand=True)

        # Top Control Bar
        top = ttk.Frame(frame, padding=5)
        top.pack(fill=tk.X)
        ttk.Button(top, text="Run Multivariate Assessment", command=self.calculate_assessment).pack(side=tk.LEFT,
                                                                                                    padx=5)
        ttk.Button(top, text="Export Assessment CSV", command=self.export_assessment_csv).pack(side=tk.LEFT, padx=5)

        ttk.Label(top, text="(Click any dot in the plot to see WHY it is an outlier)",
                  font=("Arial", 9, "italic")).pack(side=tk.LEFT, padx=15)

        # Main Split: Top (Table + Plot) vs Bottom (Contribution Analysis)
        main_paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        main_paned.pack(fill=tk.BOTH, expand=True, pady=5)

        top_pane = ttk.Frame(main_paned)
        self.bottom_pane = ttk.Frame(main_paned)  # For Contribution Plot

        main_paned.add(top_pane, weight=3)
        main_paned.add(self.bottom_pane, weight=2)

        # Top Split: Table (Left) vs Scatter (Right)
        sub_paned = ttk.PanedWindow(top_pane, orient=tk.HORIZONTAL)
        sub_paned.pack(fill=tk.BOTH, expand=True)

        left_pane = ttk.Frame(sub_paned)
        right_pane = ttk.Frame(sub_paned)
        sub_paned.add(left_pane, weight=1)
        sub_paned.add(right_pane, weight=2)

        # --- Table Setup ---
        self.txt_assess = tk.Text(left_pane, height=8, font=("Consolas", 9))
        self.txt_assess.pack(fill=tk.X, padx=5, pady=5)

        cols = ("Rank", "SN", "M-Dist", "Univar", "Fails")
        self.tree_rank = ttk.Treeview(left_pane, columns=cols, show="headings")
        for c in cols:
            self.tree_rank.heading(c, text=c)
            w = 50 if c == "Rank" else 80
            self.tree_rank.column(c, width=w, anchor="center")

        vsb = ttk.Scrollbar(left_pane, orient="vertical", command=self.tree_rank.yview)
        self.tree_rank.configure(yscrollcommand=vsb.set)
        self.tree_rank.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)

        # --- Assessment Scatter Setup ---
        self.assess_fig = plt.Figure(figsize=(6, 4), dpi=100)
        self.assess_canvas = FigureCanvasTkAgg(self.assess_fig, master=right_pane)
        self.assess_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        # CRITICAL: Bind the click
        self.assess_canvas.mpl_connect('button_press_event', self.on_assess_click)

        # --- Bottom Pane: Contribution Plot Setup ---
        self.contrib_fig = plt.Figure(figsize=(8, 3), dpi=100)
        self.contrib_canvas = FigureCanvasTkAgg(self.contrib_fig, master=self.bottom_pane)
        self.contrib_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Placeholder text
        ax = self.contrib_fig.add_subplot(111)
        ax.text(0.5, 0.5, "Click a unit in the scatter plot above to analyze its drivers.",
                ha='center', va='center', color='gray')
        ax.axis('off')

    def _setup_trend_tab(self):
        frame = ttk.Frame(self.tab_trend)
        frame.pack(fill=tk.BOTH, expand=True)

        # --- Control Bar ---
        ctrl = ttk.Frame(frame, padding=5)
        ctrl.pack(fill=tk.X)

        # Parameter Selection
        ttk.Label(ctrl, text="Parameter:").pack(side=tk.LEFT)
        self.combo_trend_param = ttk.Combobox(ctrl, width=25, state="readonly")
        self.combo_trend_param.pack(side=tk.LEFT, padx=5)

        # [NEW] Auto-trigger plot when parameter changes
        self.combo_trend_param.bind("<<ComboboxSelected>>", lambda e: self.plot_detailed_trend())

        # Analysis Type Selection
        ttk.Label(ctrl, text="Analysis Type:").pack(side=tk.LEFT, padx=(15, 2))
        self.var_trend_type = tk.StringVar(value="Mean & Cpk Trend")
        self.combo_trend_type = ttk.Combobox(ctrl, textvariable=self.var_trend_type, state="readonly", width=30)
        self.combo_trend_type["values"] = (
            "Mean & Cpk Trend",
            "Box Plot Series (Spread)",
            "Spaghetti Plot (Individual Tracks)",
            "Drift / Delta Analysis (vs T0)",
            "Shift Scatter (Final vs Initial)",  # <--- NEW OPTION
            "Tempco Scatter (Hot - Cold)"
        )
        self.combo_trend_type.pack(side=tk.LEFT, padx=5)
        self.combo_trend_type.current(0)

        # [NEW] Auto-trigger plot when graph type changes
        self.combo_trend_type.bind("<<ComboboxSelected>>", lambda e: self.plot_detailed_trend())

        # Keep the button as a backup
        ttk.Button(ctrl, text="Refresh Plot", command=self.plot_detailed_trend).pack(side=tk.LEFT, padx=15)

        self.trend_plot_frame = ttk.Frame(frame)
        self.trend_plot_frame.pack(fill=tk.BOTH, expand=True)

    def _setup_report_tab(self):
        frame = ttk.Frame(self.tab_report)
        frame.pack(fill=tk.BOTH, expand=True)

        # --- Section 1: Top Controls (PDF Generation) ---
        ctrl_frame = ttk.Frame(frame, padding=10)
        ctrl_frame.pack(fill=tk.X)

        # Big Button for the Professional Report
        btn_pdf = ttk.Button(
            ctrl_frame,
            text="Generate Professional PDF Report",
            command=self.generate_full_pdf_report
        )
        btn_pdf.pack(pady=5, ipadx=10, ipady=5)

        ttk.Label(ctrl_frame, text="(Generates a multi-page PDF with charts, tables, and outlier analysis)",
                  font=("Arial", 9, "italic")).pack(pady=(0, 10))

        ttk.Separator(frame, orient='horizontal').pack(fill='x', padx=10, pady=5)

        # --- Section 2: On-Screen Report (Original Functionality) ---
        ttk.Label(frame, text="Quick Screen Report", font=("Arial", 10, "bold")).pack(pady=(10, 2))

        ttk.Button(frame, text="Generate Screen Report", command=self.generate_screen_report).pack(pady=5)

        # Scrollable Area for Screen Report
        # Note: We use a separate frame for the canvas to handle the scrollbar layout correctly
        canvas_container = ttk.Frame(frame)
        canvas_container.pack(fill=tk.BOTH, expand=True)

        self.report_canvas = tk.Canvas(canvas_container)
        sb = ttk.Scrollbar(canvas_container, orient="vertical", command=self.report_canvas.yview)

        self.report_frame = ttk.Frame(self.report_canvas)
        self.report_frame.bind(
            "<Configure>",
            lambda e: self.report_canvas.configure(scrollregion=self.report_canvas.bbox("all")),
        )

        self.report_canvas.create_window((0, 0), window=self.report_frame, anchor="nw")
        self.report_canvas.configure(yscrollcommand=sb.set)

        self.report_canvas.pack(side="left", fill="both", expand=True)
        sb.pack(side="right", fill="y")

    def _setup_visuals_tab(self):
        frame = ttk.Frame(self.tab_visuals)
        frame.pack(fill=tk.BOTH, expand=True)
        ctrl = ttk.Frame(frame)
        ctrl.pack(fill=tk.X)

        ttk.Label(ctrl, text="Param:").pack(side=tk.LEFT)
        self.combo_param_viz = ttk.Combobox(ctrl, width=30, state="readonly")
        self.combo_param_viz.pack(side=tk.LEFT, padx=5)

        ttk.Label(ctrl, text="X-Axis:").pack(side=tk.LEFT, padx=(15, 2))
        self.combo_xmode = ttk.Combobox(ctrl, width=8, state="readonly", textvariable=self.var_xmode)
        self.combo_xmode["values"] = ("Sheet", "SN")
        self.combo_xmode.current(0)
        self.combo_xmode.pack(side=tk.LEFT)

        ttk.Label(ctrl, text="Show Limits:").pack(side=tk.LEFT, padx=(15, 2))
        self.combo_limits = ttk.Combobox(ctrl, width=5, state="readonly", textvariable=self.var_show_limits)
        self.combo_limits["values"] = ("Off", "On")
        self.combo_limits.current(0)
        self.combo_limits.pack(side=tk.LEFT)

        ttk.Button(ctrl, text="Plot Distributions", command=self.plot_distribution).pack(side=tk.LEFT, padx=10)

        self.viz_frame = ttk.Frame(frame)
        self.viz_frame.pack(fill=tk.BOTH, expand=True)

    def plot_corr_scatter(self, px, py):
        """Draw the detailed regression plot on the right panel."""
        try:
            if self.corr_scatter_canvas is None:
                self.corr_fig_scatter = plt.Figure(figsize=(5, 5), dpi=100)
                self.corr_scatter_canvas = FigureCanvasTkAgg(self.corr_fig_scatter, master=self.corr_scatter_frame)
                self.corr_scatter_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            else:
                self.corr_fig_scatter.clf()

            ax = self.corr_fig_scatter.add_subplot(111)

            data = self.current_corr_raw_data[[px, py]].dropna()
            r_val = self.current_corr_df.loc[px, py]

            # Regression Plot
            sns.regplot(
                data=data,
                x=px,
                y=py,
                ax=ax,
                scatter_kws={'alpha': 0.5, 's': 30, 'edgecolor': 'black'},
                line_kws={'color': 'red'}
            )

            ax.set_title(f"Relationship Detail\nR = {r_val:.4f}", fontweight="bold")
            ax.set_xlabel(px)
            ax.set_ylabel(py)
            ax.grid(True, alpha=0.3)

            # Annotate sample size
            ax.text(0.05, 0.95, f"N = {len(data)}", transform=ax.transAxes,
                    bbox=dict(facecolor='white', alpha=0.8))

            self.corr_fig_scatter.tight_layout()
            self.corr_scatter_canvas.draw()
        except Exception as e:
            messagebox.showerror("Plot Error", f"Could not plot scatter:\n{str(e)}")

    # ----------------- Structure detection -----------------
    def _detect_struct(self, df):
        col0 = df.iloc[:, 0].astype(str).str.strip()
        col0_lower = col0.str.lower()

        # Data start
        start = 18
        for i in range(len(col0_lower) - 1):
            if col0_lower.iloc[i] in ["1", "1.0"] and col0_lower.iloc[i + 1] in ["2", "2.0"]:
                start = i
                break
        self.var_data_start.set(start)

        # Data end
        end = len(df) - 1
        for i in range(start, len(col0_lower)):
            if col0_lower.iloc[i] in ["nan", "none", ""]:
                end = i - 1
                break
        self.var_data_end.set(end)

        search_top = max(0, start - 25)
        search_slice = col0_lower.iloc[search_top:start]

        def find_best_row(patterns_include, patterns_exclude=()):
            mask = search_slice.apply(
                lambda s: any(p in s for p in patterns_include) and not any(e in s for e in patterns_exclude)
            )
            idx = np.where(mask.to_numpy())[0]
            if len(idx):
                return search_top + int(idx[-1])
            return None

        max_row = find_best_row(
            ["max limit", "upper limit", "usl"],
            ["max reading", "maximum reading"],
        )
        min_row = find_best_row(
            ["min limit", "lower limit", "lsl"],
            ["min reading", "minimum reading"],
        )

        if max_row is None or min_row is None:
            u, l = 12, 13
            for i in range(search_top, start):
                t = col0_lower.iloc[i]
                if "max" in t and "reading" not in t:
                    u = i
                elif "min" in t and "reading" not in t:
                    l = i
            if max_row is None:
                max_row = u
            if min_row is None:
                min_row = l

        self.var_usl_row.set(max_row)
        self.var_lsl_row.set(min_row)
        header_row = max(min(max_row, min_row) - 1, 0)
        self.var_header_row.set(header_row)

    # ----------------- File loading / parsing -----------------
    def load_file(self):
        f = filedialog.askopenfilename(filetypes=[("Excel Files", "*.xlsx *.xls")])
        if not f:
            return
        self.current_file = f
        self._load_from_path(f)

    def reload_last_file(self):
        if not self.current_file or not os.path.isfile(self.current_file):
            messagebox.showinfo("Info", "No previous file found.")
            return
        self._load_from_path(self.current_file)

    def _load_from_path(self, path):
        self.lbl_status.config(text=f"Loading: {os.path.basename(path)}")
        self.root.update()
        try:
            xls = pd.ExcelFile(path)
            self.raw_previews = {
                s: pd.read_excel(xls, sheet_name=s, header=None, engine="openpyxl")
                for s in xls.sheet_names
            }
            if not xls.sheet_names:
                raise ValueError("Workbook has no sheets.")

            df0 = self.raw_previews[xls.sheet_names[0]]
            self._detect_struct(df0)

            self.combo_inspect_sheet["values"] = xls.sheet_names
            if xls.sheet_names:
                self.combo_inspect_sheet.current(0)

            self.reparse_file()
        except Exception as e:
            messagebox.showerror("Error", str(e))
            self.lbl_status.config(text="Error during load.")

    def reparse_file(self):
        if not self.current_file:
            return
        self.page_size = max(10, self.var_page_size.get() or 100)

        h, u, l = self.var_header_row.get(), self.var_usl_row.get(), self.var_lsl_row.get()
        d_s, d_e = self.var_data_start.get(), self.var_data_end.get()
        sn_idx = self.var_sn_col.get()

        self.data_dict.clear()
        self.limits_dict.clear()
        self.param_map.clear()
        self.failures.clear()
        self.measure_long = []
        self.limit_long = []

        try:
            for sheet, raw in self.raw_previews.items():
                if h >= len(raw):
                    continue

                raw_header = raw.iloc[h].values
                clean_header = self._process_headers(raw_header)

                usls = pd.to_numeric(raw.iloc[u].values, errors="coerce") if u < len(raw) else []
                lsls = pd.to_numeric(raw.iloc[l].values, errors="coerce") if l < len(raw) else []

                e = d_e if d_e > d_s else len(raw) - 1
                e = min(e, len(raw) - 1)
                df = raw.iloc[d_s : e + 1, :].copy()

                valid = min(len(clean_header), df.shape[1])
                df = df.iloc[:, :valid]

                col_names = list(clean_header[:valid])
                if sn_idx < len(col_names):
                    col_names[sn_idx] = "SN"
                df.columns = col_names

                df = df.dropna(subset=["SN"], how="all")
                df["SN"] = df["SN"].astype(str)

                for c in df.columns:
                    if c != "SN":
                        df[c] = pd.to_numeric(df[c], errors="coerce")

                self.data_dict[sheet] = df

                s_lims = {}
                for i, p in enumerate(clean_header[:valid]):
                    if i == sn_idx:
                        continue
                    if p not in df.columns:
                        continue

                    base = self._get_base(p)
                    if base not in self.param_map:
                        self.param_map[base] = []
                    if sheet not in self.param_map[base]:
                        self.param_map[base].append(sheet)

                    uv = usls[i] if i < len(usls) else np.nan
                    lv = lsls[i] if i < len(lsls) else np.nan
                    s_lims[p] = (lv, uv)

                    self.limit_long.append(
                        {"Sheet": sheet, "Parameter": p, "LSL": lv, "USL": uv}
                    )

                self.limits_dict[sheet] = s_lims

                value_cols = [c for c in df.columns if c != "SN"]
                melted = df.melt(id_vars=["SN"], value_vars=value_cols, var_name="Parameter", value_name="Value")
                melted["Sheet"] = sheet
                self.measure_long.append(melted)

            if not self.measure_long:
                self.lbl_status.config(text="No data parsed.")
                return

            self.measure_long = pd.concat(self.measure_long, ignore_index=True)
            self.limit_long = pd.DataFrame(self.limit_long).drop_duplicates()

            merged = pd.merge(
                self.measure_long,
                self.limit_long,
                on=["Sheet", "Parameter"],
                how="left",
            )

            for col in ["Value", "LSL", "USL"]:
                merged[col] = pd.to_numeric(merged[col], errors='coerce')

            merged["FailHigh"] = (~merged["USL"].isna()) & (merged["Value"] > merged["USL"])
            merged["FailLow"]  = (~merged["LSL"].isna()) & (merged["Value"] < merged["LSL"])
            merged["FailType"] = np.where(
                merged["FailHigh"], "High",
                np.where(merged["FailLow"], "Low", "")
            )

            self.fail_long = merged[(merged["FailHigh"] | merged["FailLow"]) & (~merged["Value"].isna())]

            self.failures = []
            for _, r in self.fail_long.iterrows():
                self.failures.append(
                    {
                        "Sheet": r["Sheet"],
                        "SN": r["SN"],
                        "Parameter": r["Parameter"],
                        "Value": r["Value"],
                        "LSL": r["LSL"],
                        "USL": r["USL"],
                        "Type": r["FailType"],
                    }
                )

            self.combo_trend_param["values"] = sorted(list(self.param_map.keys()))
            self.combo_param_viz["values"] = self.combo_trend_param["values"]

            self.current_page = 0
            self.update_inspector_grid(None)
            self.plot_cpk_viz()
            self.populate_failures()
            self.calculate_assessment()
            self.lbl_status.config(text="Analysis Complete.")

            if hasattr(self, 'combo_corr_sheet'):
                self.combo_corr_sheet['values'] = list(self.data_dict.keys())
                if self.data_dict:
                    self.combo_corr_sheet.current(0)

        except Exception as e:
            messagebox.showerror("Error", str(e))
            self.lbl_status.config(text="Error during parse.")

    # ----------------- Inspector pagination -----------------
    def update_inspector_grid(self, _event):
        sheet = self.combo_inspect_sheet.get()
        if sheet not in self.raw_previews:
            return
        df = self.raw_previews[sheet]
        self.total_rows = len(df)
        self.lbl_page.config(text=f"Page {self.current_page + 1}")
        self.page_size = max(10, self.var_page_size.get() or 100)

        self.tree_raw.delete(*self.tree_raw.get_children())
        self.tree_raw["columns"] = list(range(len(df.columns)))

        start = self.current_page * self.page_size
        subset = df.iloc[start : start + self.page_size]

        w = int(self.var_col_width.get())

        for i in range(len(df.columns)):
            self.tree_raw.heading(i, text=str(i))
            # stretch=False ensures manual width is honored
            self.tree_raw.column(i, width=w, stretch=False, anchor="w")

        h = self.var_header_row.get()
        u, l = self.var_usl_row.get(), self.var_lsl_row.get()
        ds, de = self.var_data_start.get(), self.var_data_end.get()

        for i, row in subset.iterrows():
            vals = [str(x) if not pd.isna(x) else "" for x in row]
            tag = ""
            if i == h:
                tag = "HEADER"
            elif i == u:
                tag = "USL"
            elif i == l:
                tag = "LSL"
            elif i == ds:
                tag = "DATA_START"
            elif de > 0 and i == de:
                tag = "DATA_END"
            self.tree_raw.insert("", "end", values=vals, tags=(tag,))

        self._apply_inspector_row_height()
        self.tree_raw.update_idletasks()

    def prev_page(self):
        if self.current_page > 0:
            self.current_page -= 1
            self.update_inspector_grid(None)

    def next_page(self):
        sheet = self.combo_inspect_sheet.get()
        if sheet in self.raw_previews:
            total = len(self.raw_previews[sheet])
            max_page = max(0, (total - 1) // self.page_size)
            if self.current_page < max_page:
                self.current_page += 1
                self.update_inspector_grid(None)

    def goto_page(self):
        sheet = self.combo_inspect_sheet.get()
        if sheet not in self.raw_previews:
            return
        total = len(self.raw_previews[sheet])
        max_page = max(0, (total - 1) // self.page_size)
        p = max(0, min(max_page, self.var_goto_page.get() - 1))
        self.current_page = p
        self.update_inspector_grid(None)

    def on_sheet_change(self, _event):
        self.current_page = 0
        self.update_inspector_grid(None)

    # ----------------- Cpk plotting -----------------
    def plot_cpk_viz(self):
        # 1. Gather Data
        data = []
        for s, df in self.data_dict.items():
            for c in df.columns:
                if c == "SN": continue
                v = df[c].dropna()
                l, u = self.limits_dict[s].get(c, (np.nan, np.nan))
                cpk = self._calc_robust_cpk(v, l, u)

                if not pd.isna(cpk):
                    # Calculate "Centering" (0.0 = LSL, 1.0 = USL, 0.5 = Perfect Center)
                    centering = np.nan
                    if not pd.isna(l) and not pd.isna(u) and (u != l):
                        mean_val = v.mean()
                        centering = (mean_val - l) / (u - l)

                    data.append({
                        "Sheet": s,
                        "Param": c,
                        "Cpk": cpk,
                        "Mean": v.mean(),
                        "Centering": centering
                    })

        if not data: return
        df_p = pd.DataFrame(data)
        df_p = df_p.replace([np.inf, -np.inf], np.nan).dropna(subset=["Cpk"])
        if df_p.empty: return

        # 2. Update Treeview (Standard for all views)
        self.tree_cpk.delete(*self.tree_cpk.get_children())
        for _, r in df_p.sort_values("Cpk").iterrows():
            lbl = f"{r['Sheet']}:{r['Param']}"
            self.tree_cpk.insert("", "end", values=(lbl, f"{r['Cpk']:.3f}", f"{r['Mean']:.3f}"))

        # 3. Handle Canvas
        if self.cpk_canvas is None:
            self.cpk_fig = plt.Figure(figsize=(12, 6), dpi=100)  # Use Figure, not subplots, for easier clearing
            self.cpk_canvas = FigureCanvasTkAgg(self.cpk_fig, master=self.cpk_plot_frame)
            self.cpk_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        else:
            self.cpk_fig.clf()

        # 4. Dispatch Plot Type
        mode = self.var_cpk_viz_mode.get()
        ax = self.cpk_fig.add_subplot(111)

        # --- MODE A: Histogram (Classic) ---
        if "Histogram" in mode:
            viz_max = 4.0
            clipped = df_p["Cpk"].clip(upper=viz_max)
            ax.hist(clipped, bins=25, range=(0, viz_max), color="#4C72B0", edgecolor="white")
            ax.axvline(1.33, color="orange", ls="--", label="1.33 Limit")
            ax.axvline(1.67, color="green", ls="--", label="1.67 Limit")
            ax.set_title("Overall Cpk Distribution", fontsize=12, fontweight='bold')
            ax.set_xlabel(f"Cpk (Capped at {viz_max})")
            ax.legend()

        # --- MODE B: Box Plot (Compare Test Steps) ---
        elif "Box Plot" in mode:
            # FIX 1: Clip extreme values at 10.0 instead of letting the plot cut them off.
            # Cpk > 10 is statistically "Infinity" for a Qual.
            # This ensures even "perfect" parameters are visible at the top of the chart.
            plot_data = df_p.copy()
            plot_data["Cpk"] = plot_data["Cpk"].clip(upper=10.0)

            # Create the box plot using this "safe" data
            sns.boxplot(data=plot_data, x="Sheet", y="Cpk", ax=ax, palette="vlag")
            sns.stripplot(data=plot_data, x="Sheet", y="Cpk", ax=ax, color="black", alpha=0.3, jitter=True, size=3)

            # Add the limit line
            ax.axhline(1.33, color="red", ls="--", alpha=0.5, label="1.33 Limit")

            # FIX 2: Remove the hard "5" limit.
            # Instead, set the top to slightly above the highest value in the data (max 10).
            # This "Auto-fits" the chart to your actual data height.
            current_max = plot_data["Cpk"].max()
            ax.set_ylim(0, current_max * 1.1)

            ax.set_title("Process Capability by Electrical Test Step", fontsize=12, fontweight='bold')
            ax.set_xlabel("Test Step / Sheet")

        # --- MODE C: Bubble Plot (Outlier Spotter) ---
        elif "Bubble" in mode:
            # X = Sheet (Categorical mapped to Int), Y = Cpk
            sheets = sorted(df_p["Sheet"].unique())
            sheet_map = {name: i for i, name in enumerate(sheets)}
            df_p["SheetIdx"] = df_p["Sheet"].map(sheet_map)

            # Size: Inverse of Cpk (Smaller Cpk = Bigger Risk Bubble)
            # We clip the denominator so Cpk=0 doesn't cause div/0 or infinite size
            # We also cap the Cpk used for sizing at 3.0, so "Good" points (Cpk>3) represent minimum risk size
            size_cpk = df_p["Cpk"].clip(lower=0.1, upper=3.0)
            sizes = (1.0 / size_cpk) * 250

            # Color map: Red (Bad) to Green (Good)
            # We clip color data at 2.0 so super-high Cpk doesn't skew the color scale
            color_data = df_p["Cpk"].clip(upper=2.0)

            sc = ax.scatter(
                df_p["SheetIdx"],
                df_p["Cpk"],
                s=sizes,
                c=color_data,
                cmap="RdYlGn",
                alpha=0.6,
                edgecolors="black",
                linewidth=0.5
            )

            # --- THE FIX: SYMLOG SCALE ---
            # 'linthresh=3' means: Be Linear from 0 to 3. Be Logarithmic above 3.
            # This keeps your 0-2 range perfectly readable while shrinking the 10,000 outlier.
            ax.set_yscale('symlog', linthresh=3)

            # Force specific y-ticks so the linear part is readable
            # We want to see 0, 1, 1.33, 2, 3 clearly.
            # Then powers of 10 for the outliers.
            from matplotlib.ticker import FixedLocator, ScalarFormatter

            # Combine standard linear ticks with log ticks
            major_ticks = [0, 1, 1.33, 2, 3, 10, 100, 1000, 10000]
            # Filter ticks to only those within actual data range (plus a buffer)
            max_val = df_p["Cpk"].max()
            visible_ticks = [t for t in major_ticks if t <= max_val * 10]

            ax.yaxis.set_major_locator(FixedLocator(visible_ticks))
            ax.yaxis.set_major_formatter(ScalarFormatter())  # Formatting numbers as 10, 100 (not 10^2)

            ax.set_xticks(range(len(sheets)))
            ax.set_xticklabels(sheets, rotation=45, ha="right")
            ax.set_title("Cpk Outlier Detection (Symlog Scale)", fontsize=12, fontweight='bold')
            ax.set_ylabel("Cpk Value (Linear â‰¤ 3, Log > 3)")

            # Add grid for both major (labeled) and minor ticks
            ax.grid(True, which='major', linestyle="-", alpha=0.5)
            ax.grid(True, which='minor', linestyle=":", alpha=0.2)

            # Add colorbar
            plt.colorbar(sc, ax=ax, label="Cpk (Color capped at 2.0)")

        # --- MODE D: Centering Analysis (The Engineer's Favorite) ---
        elif "Centering" in mode:
            # Filter for items that actually have limits (Centering != NaN)
            valid = df_p.dropna(subset=["Centering"])
            if valid.empty:
                ax.text(0.5, 0.5, "No parameters with both LSL & USL found", ha='center')
            else:
                sc = ax.scatter(
                    valid["Centering"],
                    valid["Cpk"],
                    c=valid["Cpk"],
                    cmap="RdYlGn",
                    edgecolors="grey",
                    alpha=0.7
                )

                # Draw "Goal Posts"
                ax.axvline(0.0, color="red", ls="-", label="LSL")
                ax.axvline(1.0, color="red", ls="-", label="USL")
                ax.axvline(0.5, color="grey", ls=":", label="Target")

                ax.set_xlim(-0.2, 1.2)
                ax.set_ylim(0, 4)
                ax.set_title("Centering Analysis: Are failures due to Noise or Drift?", fontsize=12, fontweight='bold')
                ax.set_xlabel("Mean Position (0=LSL, 0.5=Center, 1=USL)")
                ax.set_ylabel("Cpk (Precision)")
                ax.legend(loc='upper right')

        self.cpk_fig.tight_layout()
        self.cpk_canvas.draw()

    # ----------------- Detailed Trend (no 3Ïƒ shading) -----------------
    def plot_detailed_trend(self):
        try:
            param = self.combo_trend_param.get()
            plot_type = self.var_trend_type.get()

            # 1. Clear previous plot immediately to prevent "stale" data confusion
            if self.trend_canvas is None:
                self.trend_fig = plt.Figure(figsize=(10, 6), dpi=100)
                self.trend_canvas = FigureCanvasTkAgg(self.trend_fig, master=self.trend_plot_frame)
                self.trend_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            else:
                self.trend_fig.clf()

            # Force a draw now so if we return early, the user sees a blank screen (not the old plot)
            self.trend_canvas.draw()

            if not param or self.measure_long is None:
                return

            # 2. Global Definition of sheet_order (CRITICAL FIX)
            # This must exist before any if/else blocks try to use it
            sheet_order = list(self.data_dict.keys())

            # 3. Create the Base Subset (for Box/Spaghetti plots)
            # We filter by the base parameter name to capture all temps
            subset = self.measure_long[
                self.measure_long["Parameter"].apply(lambda x: self._get_base(x) == param)].copy()

            if subset.empty:
                # If no data found, just exit (screen is already cleared)
                return

            # Sort subset by Sheet order
            subset["Sheet"] = subset["Sheet"].astype(str)
            subset["Sheet"] = pd.Categorical(subset["Sheet"], categories=sheet_order, ordered=True)
            subset = subset.sort_values("Sheet")

            ax = self.trend_fig.add_subplot(111)

            # =========================================================
            # MODE 1: Mean & Cpk Trend (Tri-Temp Robust)
            # =========================================================
            if "Mean & Cpk" in plot_type:
                ax2 = ax.twinx()
                styles = {
                    "Room": ("green", "o", "-"),
                    "Cold": ("blue", "D", "--"),
                    "Hot": ("red", "s", ":"),
                    "Data": ("black", "x", "-.")
                }

                def get_temp(p_name):
                    if "(Room)" in p_name: return "Room"
                    if "(Cold)" in p_name: return "Cold"
                    if "(Hot)" in p_name: return "Hot"
                    return "Data"

                # Find specific columns (e.g., Icc Room, Icc Hot)
                all_params = self.measure_long["Parameter"].unique()
                relevant_cols = [p for p in all_params if self._get_base(p) == param]

                # Gather data by temp
                temp_groups = {}
                for col in relevant_cols:
                    temp = get_temp(col)
                    if temp not in temp_groups:
                        temp_groups[temp] = {"sheets": [], "means": [], "cpks": []}

                    for s in sheet_order:
                        temp_groups[temp]["sheets"].append(s)
                        df = self.data_dict[s]
                        if col in df.columns:
                            v = df[col].dropna()
                            if not v.empty:
                                temp_groups[temp]["means"].append(v.mean())
                                l, u = self.limits_dict[s].get(col, (np.nan, np.nan))
                                cpk = self._calc_robust_cpk(v, l, u)
                                temp_groups[temp]["cpks"].append(cpk)
                            else:
                                temp_groups[temp]["means"].append(np.nan)
                                temp_groups[temp]["cpks"].append(np.nan)
                        else:
                            temp_groups[temp]["means"].append(np.nan)
                            temp_groups[temp]["cpks"].append(np.nan)

                # Plot
                lines, labels = [], []
                for temp, data in temp_groups.items():
                    # Skip if no data
                    if all(pd.isna(x) for x in data["means"]): continue

                    c, mk, ls = styles.get(temp, styles["Data"])
                    l1 = ax.plot(data["sheets"], data["means"], color=c, marker=mk, ls="-", alpha=0.8,
                                 label=f"{temp} Mean")
                    l2 = ax2.plot(data["sheets"], data["cpks"], color=c, marker=mk, ls=ls, lw=1.5, alpha=0.5,
                                  label=f"{temp} Cpk")
                    lines.extend(l1 + l2)
                    labels.extend([l1[0].get_label(), l2[0].get_label()])

                ax.set_title(f"Tri-Temp Trend: {param}")
                ax.set_ylabel("Mean Value")
                ax2.set_ylabel("Cpk (Dashed)")
                ax2.axhline(1.33, color="gray", ls="--", alpha=0.5)
                if lines:
                    ax.legend(lines, labels, loc='upper left', fontsize=8, ncol=2)
                ax.grid(True, alpha=0.3)

            # =========================================================
            # MODE 2: Box Plot (Spread)
            # =========================================================
            elif "Box Plot" in plot_type:
                sns.boxplot(data=subset, x="Sheet", y="Value", hue="Sheet", legend=False, ax=ax, palette="Blues",
                            showfliers=False)
                sns.stripplot(data=subset, x="Sheet", y="Value", ax=ax, color="black", alpha=0.3, size=2, jitter=True)
                ax.set_title(f"Distribution Trend: {param}")
                ax.set_xlabel("Readpoint")
                ax.grid(True, axis='y', alpha=0.5)

            # =========================================================
            # MODE 3: Spaghetti Plot (Maverick Detection)
            # =========================================================
            elif "Spaghetti" in plot_type:
                unique_sn = subset["SN"].nunique()
                alpha_val = 0.5 if unique_sn < 50 else 0.1
                sns.lineplot(data=subset, x="Sheet", y="Value", hue="SN", estimator=None, units="SN", ax=ax,
                             palette="dark:grey", legend=False, alpha=alpha_val, lw=1)

                # Highlight Mean
                sns.lineplot(data=subset, x="Sheet", y="Value", ax=ax, color="red", lw=3, errorbar=None, label="Mean")
                ax.set_title(f"Individual Unit Trajectories (N={unique_sn})")
                ax.set_xlabel("Readpoint")
                ax.legend(loc='upper right')

            # =========================================================
            # MODE 4: Drift Analysis (Robust)
            # =========================================================
            elif "Drift" in plot_type:
                # Identify correct columns (Room/Cold/Hot) for this param
                all_params = self.measure_long["Parameter"].unique()
                relevant_cols = [p for p in all_params if self._get_base(p) == param]
                drift_subset = self.measure_long[self.measure_long["Parameter"].isin(relevant_cols)].copy()

                if drift_subset.empty: return

                drift_subset["Sheet"] = drift_subset["Sheet"].astype(str)
                pivoted = drift_subset.pivot_table(index="SN", columns="Sheet", values="Value", aggfunc='first')

                valid_sheets = [s for s in sheet_order if s in pivoted.columns]
                if len(valid_sheets) < 2:
                    ax.text(0.5, 0.5, "Insufficient timepoints for drift analysis", ha='center')
                else:
                    pivoted = pivoted[valid_sheets]
                    inc_drift = pivoted.diff(axis=1)
                    t0 = pivoted.iloc[:, 0]
                    cum_drift = pivoted.subtract(t0, axis=0)
                    cum_mean = cum_drift.mean(axis=0)

                    melted = inc_drift.reset_index().melt(id_vars="SN", var_name="Sheet", value_name="Drift")

                    ax2 = ax.twinx()
                    ax.axhline(0, color='black', alpha=0.3)
                    sns.boxplot(data=melted, x="Sheet", y="Drift", hue="Sheet", legend=False, ax=ax, palette="coolwarm",
                                showfliers=False, boxprops=dict(alpha=0.6))

                    ax2.plot(range(len(valid_sheets)), cum_mean.values, color="purple", marker="D", ls="--", lw=2,
                             label="Cumulative Mean Drift")

                    ax.set_title(f"Step-by-Step vs Cumulative Drift: {param}")
                    ax.set_ylabel("Incremental Drift (Step N - N-1)")
                    ax2.set_ylabel("Total Mean Drift (vs T0)", color="purple")
                    ax2.tick_params(axis='y', labelcolor="purple")
                    ax2.legend(loc='upper left')
                    ax.grid(True, alpha=0.3)

            # =========================================================
            # MODE 5: Shift Scatter (Headroom Analysis)
            # =========================================================
            elif "Shift Scatter" in plot_type:
                # 1. Pivot Data to get Columns = Readpoints
                all_params = self.measure_long["Parameter"].unique()
                relevant_cols = [p for p in all_params if self._get_base(p) == param]
                shift_subset = self.measure_long[self.measure_long["Parameter"].isin(relevant_cols)].copy()

                if shift_subset.empty: return

                shift_subset["Sheet"] = shift_subset["Sheet"].astype(str)
                # Pivot: Index=SN, Columns=Sheet, Values=Value
                pivoted = shift_subset.pivot_table(index="SN", columns="Sheet", values="Value", aggfunc='first')

                # Filter to valid chronological sheets
                valid_sheets = [s for s in sheet_order if s in pivoted.columns]
                if len(valid_sheets) < 2:
                    ax.text(0.5, 0.5, "Need at least 2 readpoints (Initial & Final) to plot shift.", ha='center')
                    self.trend_canvas.draw()
                    return

                # 2. Define Initial (T0) and Final (Tx)
                # We compare the FIRST recorded step vs the LAST recorded step
                col_initial = valid_sheets[0]
                col_final = valid_sheets[-1]

                data_x = pivoted[col_initial]
                data_y = pivoted[col_final]

                # 3. Get Limits (for the "Box")
                # We try to find limits from the Final sheet first
                l, u = self.limits_dict[col_final].get(param, (np.nan, np.nan))
                # If specific param name fails, try searching the 'relevant_cols' list for a match
                if pd.isna(l) and pd.isna(u):
                    # Try to grab limits from the first variant found
                    for c in relevant_cols:
                        tl, tu = self.limits_dict[col_final].get(c, (np.nan, np.nan))
                        if not pd.isna(tl): l, u = tl, tu; break

                # 4. Plotting
                # Plot the Identity Line (No Drift)
                min_val = min(data_x.min(), data_y.min())
                max_val = max(data_x.max(), data_y.max())

                # Add 5% buffer for visuals
                span = max_val - min_val
                if span == 0: span = 0.1
                plot_min = min_val - span * 0.1
                plot_max = max_val + span * 0.1

                # Reference Line (y=x)
                ax.plot([plot_min, plot_max], [plot_min, plot_max], color='gray', linestyle='--', alpha=0.5,
                        label="No Drift (y=x)")

                # Scatter Points
                # Color code: Drift Up (Red), Drift Down (Blue)
                drift = data_y - data_x
                colors = ['#d62728' if d > 0 else '#1f77b4' for d in drift]

                scatter = ax.scatter(data_x, data_y, c=colors, edgecolors='black', alpha=0.7, s=40)

                # 5. Draw the "Spec Box" (if limits exist)
                if not pd.isna(l):
                    ax.axvline(l, color='red', linestyle='-', linewidth=1)  # Initial LSL
                    ax.axhline(l, color='red', linestyle='-', linewidth=1)  # Final LSL
                    ax.text(l, plot_max, "LSL", color='red', ha='right', va='top', rotation=90)

                if not pd.isna(u):
                    ax.axvline(u, color='red', linestyle='-', linewidth=1)  # Initial USL
                    ax.axhline(u, color='red', linestyle='-', linewidth=1)  # Final USL
                    ax.text(u, plot_min, "USL", color='red', ha='right', va='bottom', rotation=90)

                # 6. Labels & Stats
                ax.set_title(f"Shift Analysis: {col_final} vs {col_initial}", fontweight="bold")
                ax.set_xlabel(f"Initial Value ({col_initial})")
                ax.set_ylabel(f"Final Value ({col_final})")

                # Annotate largest drifter
                max_drift_idx = drift.abs().idxmax()
                if not pd.isna(max_drift_idx):
                    mx = data_x.loc[max_drift_idx]
                    my = data_y.loc[max_drift_idx]
                    ax.annotate(f"Max Drift: SN {max_drift_idx}",
                                xy=(mx, my), xytext=(mx, my + span * 0.05),
                                arrowprops=dict(facecolor='black', shrink=0.05),
                                ha='center', fontweight='bold')

                ax.grid(True, linestyle=':', alpha=0.6)
                ax.set_aspect('equal', adjustable='datalim')  # Square aspect ratio is crucial for y=x interpretation

            # =========================================================
            # MODE 6: Tempco Scatter (Thermal Sensitivity)
            # =========================================================
            elif "Tempco Scatter" in plot_type:
                # 1. Identify the Hot, Cold, and Room variations of this parameter
                # We need to scan the columns of the CURRENT sheet (or T0) to find the variants
                # Since Tempco is usually an intrinsic property, we can look at T0 (Sheet 0)
                # or allow the user to see how Tempco changes over time (by using the current sheet).
                # For robustness, let's calculate it for ALL sheets and use the most recent one
                # or simply pick the first sheet where both Hot/Cold exist (Time Zero).

                target_sheet = sheet_order[0]  # Default to T0 (Initial Characterization)

                # Try to find the specific column names
                # We construct the expected names based on your parser logic
                col_room = f"{param} (Room)"
                col_cold = f"{param} (Cold)"
                col_hot = f"{param} (Hot)"

                # Check if these exist in the dataset
                # Note: Your parser strips brackets sometimes, or adds them.
                # We need a robust search.

                df = self.data_dict[target_sheet].copy()
                cols = df.columns

                # Fuzzy match helper
                def find_col(substring):
                    matches = [c for c in cols if substring.lower() in c.lower() and param.lower() in c.lower()]
                    # Prefer exact matches of the base param + suffix
                    return matches[0] if matches else None

                real_room = find_col("(Room)")
                real_cold = find_col("(Cold)")
                real_hot = find_col("(Hot)")

                # Fallback: If param name itself doesn't have suffix (e.g. data was "Icc", "Icc Cold")
                if not real_room: real_room = find_col("Room") or param  # Assume base is room?
                if not real_cold: real_cold = find_col("Cold")
                if not real_hot:  real_hot = find_col("Hot")

                if not real_cold or not real_hot:
                    ax.text(0.5, 0.5, f"Cannot find Hot/Cold data for '{param}'\nin sheet '{target_sheet}'",
                            ha='center')
                    self.trend_canvas.draw()
                    return

                # 2. Extract Data
                # Filter out NaNs to ensure we have matched pairs
                data = df[["SN", real_room, real_cold, real_hot]].dropna()

                if data.empty:
                    ax.text(0.5, 0.5, "No overlapping SNs with full Tri-Temp data.", ha='center')
                    return

                # 3. Calculate Tempco (Swing)
                # We just do Delta V. If you knew degrees C, you could divide by (125 - -40).
                # For visual outlier detection, raw Delta is sufficient and safer.
                data["Swing"] = data[real_hot] - data[real_cold]

                # 4. Plot
                # X-Axis: Room Value (Magnitude)
                # Y-Axis: Thermal Swing (Sensitivity)

                # Color code by "Abnormal Swing"
                # Robust Outlier Detection: Median +/- 3 * IQR
                median_swing = data["Swing"].median()
                iqr_swing = data["Swing"].quantile(0.75) - data["Swing"].quantile(0.25)
                limit_upper = median_swing + 3 * iqr_swing
                limit_lower = median_swing - 3 * iqr_swing

                colors = []
                for s in data["Swing"]:
                    if s > limit_upper or s < limit_lower:
                        colors.append('red')
                    else:
                        colors.append('teal')

                ax.scatter(data[real_room], data["Swing"], c=colors, edgecolors='black', alpha=0.7, s=45)

                # Add reference lines
                ax.axhline(median_swing, color='gray', linestyle='--', label="Median Swing")
                ax.axhline(limit_upper, color='red', linestyle=':', alpha=0.5, label="Statistical Limit (3*IQR)")
                ax.axhline(limit_lower, color='red', linestyle=':', alpha=0.5)

                ax.set_title(f"Tempco Analysis (Thermal Sensitivity)\n{param} [Hot - Cold]", fontweight="bold")
                ax.set_xlabel(f"Room Temp Value ({real_room})")
                ax.set_ylabel(f"Thermal Swing ({real_hot} - {real_cold})")

                ax.legend()
                ax.grid(True, alpha=0.3)

                # Annotate extreme outliers
                outliers = data[(data["Swing"] > limit_upper) | (data["Swing"] < limit_lower)]
                for idx, row in outliers.iterrows():
                    ax.text(row[real_room], row["Swing"], str(row["SN"]), fontsize=8, color='darkred',
                            fontweight='bold')

            self.trend_fig.tight_layout()
            self.trend_canvas.draw()

        except Exception as e:
            # Catch crashes (like NameError or ValueError) and show them
            messagebox.showerror("Plot Error", f"An error occurred while plotting:\n{str(e)}")
            # Clear the canvas so the user knows it failed
            if self.trend_fig:
                self.trend_fig.clf()
                self.trend_canvas.draw()

    # ----------------- Distributions tab -----------------
    def plot_distribution(self):
        param = self.combo_param_viz.get()
        if not param or self.measure_long is None:
            return

        xmode = self.var_xmode.get()
        show_limits = self.var_show_limits.get() == "On"

        base_param = param
        sub = self.measure_long[self.measure_long["Parameter"].apply(lambda p: self._get_base(p) == base_param)]
        if sub.empty:
            messagebox.showinfo("Info", f"No numeric data found for {param}")
            return

        limits_sub = self.limit_long[self.limit_long["Parameter"].isin(sub["Parameter"].unique())]
        dist_df = pd.merge(sub, limits_sub, on=["Sheet", "Parameter"], how="left")

        if self.viz_canvas is None:
            self.viz_fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            self.viz_canvas = FigureCanvasTkAgg(self.viz_fig, master=self.viz_frame)
            self.viz_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        else:
            self.viz_fig.clf()
            ax1, ax2 = self.viz_fig.subplots(1, 2)

        def temp_of(p):
            for t in ["Room", "Cold", "Hot"]:
                if f"({t})" in p:
                    return t
            return "Data"

        dist_df["Temp"] = dist_df["Parameter"].apply(temp_of)

        if self.is_tritemp:
            sns.histplot(
                data=dist_df,
                x="Value",
                hue="Temp",
                kde=True,
                ax=ax1,
                palette={"Room": "green", "Cold": "blue", "Hot": "red"},
                multiple="layer",
                alpha=0.5,
            )
            ax1.set_title(f"{base_param} Distribution by Temperature")
        else:
            sns.histplot(dist_df["Value"], kde=True, ax=ax1, color="steelblue", edgecolor="black")
            ax1.set_title(f"{base_param} Overall Distribution")

        if show_limits:
            if self.is_tritemp:
                for t, c in [("Room", "green"), ("Cold", "blue"), ("Hot", "red")]:
                    tmp = dist_df[dist_df["Temp"] == t]
                    if not tmp.empty:
                        l_vals = tmp["LSL"].dropna().unique()
                        u_vals = tmp["USL"].dropna().unique()
                        if l_vals.size:
                            ax1.axvline(l_vals[0], color=c, linestyle="--", alpha=0.7)
                        if u_vals.size:
                            ax1.axvline(u_vals[0], color=c, linestyle="--", alpha=0.7)
            else:
                l_vals = dist_df["LSL"].dropna().unique()
                u_vals = dist_df["USL"].dropna().unique()
                if l_vals.size:
                    ax1.axvline(l_vals[0], color="black", linestyle="--", alpha=0.7)
                if u_vals.size:
                    ax1.axvline(u_vals[0], color="black", linestyle="--", alpha=0.7)

        if xmode == "Sheet":
            x_col = "Sheet"
            ax2.set_title(f"{base_param} by Sheet / Lot")
        else:
            x_col = "SN"
            ax2.set_title(f"{base_param} by Serial Number")

        dist_plot = dist_df.copy()
        if xmode == "SN":
            counts = dist_plot["SN"].value_counts().head(50).index
            dist_plot = dist_plot[dist_plot["SN"].isin(counts)]

        if self.is_tritemp:
            sns.boxplot(
                data=dist_plot,
                x=x_col,
                y="Value",
                hue="Temp",
                ax=ax2,
                palette={"Room": "green", "Cold": "blue", "Hot": "red"},
            )
            ax2.legend(title="Temp", fontsize=8)
        else:
            sns.boxplot(data=dist_plot, x=x_col, y="Value", ax=ax2, color="lightgray")

        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha="right")

        if show_limits:
            if self.is_tritemp:
                for t, c in [("Room", "green"), ("Cold", "blue"), ("Hot", "red")]:
                    tmp = dist_plot[dist_plot["Temp"] == t]
                    if not tmp.empty:
                        l_vals = tmp["LSL"].dropna().unique()
                        u_vals = tmp["USL"].dropna().unique()
                        if l_vals.size:
                            ax2.axhline(l_vals[0], color=c, linestyle="--", alpha=0.7)
                        if u_vals.size:
                            ax2.axhline(u_vals[0], color=c, linestyle="--", alpha=0.7)
            else:
                l_vals = dist_plot["LSL"].dropna().unique()
                u_vals = dist_plot["USL"].dropna().unique()
                if l_vals.size:
                    ax2.axhline(l_vals[0], color="black", linestyle="--", alpha=0.7)
                if u_vals.size:
                    ax2.axhline(u_vals[0], color="black", linestyle="--", alpha=0.7)

        self.viz_canvas.draw()

    # ----------------- Failures and assessment -----------------
    def populate_failures(self):
        self.tree_fail.delete(*self.tree_fail.get_children())
        for f in self.failures:
            self.tree_fail.insert(
                "",
                "end",
                values=(f["Sheet"], f["SN"], f["Parameter"], f["Value"], f["LSL"], f["USL"], f["Type"]),
            )
        self.lbl_fail_summary.config(text=f"Total Failures Detected: {len(self.failures)}")

    def on_assess_click(self, event):
        """Finds the closest unit to the click and runs contribution analysis."""
        if event.inaxes is None or not hasattr(self, 'pca_transformed'):
            return

        # Find closest SN
        # X-axis: UnivarScore, Y-axis: Mahalanobis
        df = self.pca_data_clean

        # Simple distance check in plot coordinates
        # (This is approximate but sufficient for UI clicking)
        click_x, click_y = event.xdata, event.ydata

        # Calculate Euclidean distance to every point in the plot
        distances = np.sqrt(
            (df["UnivarScore"] - click_x) ** 2 +
            (df["Mahalanobis"] - click_y) ** 2
        )

        # Get closest
        closest_idx = distances.argmin()
        closest_sn = df.index[closest_idx]
        min_dist = distances.iloc[closest_idx]

        # Threshold to ignore accidental clicks on whitespace
        # (Scale dependent, but < 5.0 usually works for these chart ranges)
        if min_dist < 10.0:
            self.plot_contribution(closest_sn)

    def plot_contribution(self, sn):
        """
        Decomposes the Mahalanobis Distance to find the 'Guilty' parameters.
        """
        try:
            # 1. Get the PCA scores for this SN
            sn_idx = self.pca_data_clean.index.get_loc(sn)
            pca_scores = self.pca_transformed[sn_idx]

            # 2. Calculate feature impact
            loadings = self.pca_model.components_
            weighted_loadings = pca_scores.reshape(-1, 1) * loadings
            feature_impact = np.abs(weighted_loadings).sum(axis=0)

            # Map back to feature names
            features = self.pca_data_clean.columns.drop(["Mahalanobis", "UnivarScore"])
            impact_series = pd.Series(feature_impact, index=features)

            # Top 10 Contributors
            top_10 = impact_series.sort_values(ascending=False).head(10)

            # 3. Plot
            self.contrib_fig.clf()
            ax = self.contrib_fig.add_subplot(111)

            # Calculate raw Z-scores for coloring (Red=High, Blue=Low)
            raw_z = (self.pca_data_clean.loc[sn, top_10.index] - self.pca_data_clean[top_10.index].mean()) / \
                    self.pca_data_clean[top_10.index].std()
            colors = ['#d62728' if z > 0 else '#1f77b4' for z in raw_z]

            # --- FIXED SEABORN CALL ---
            sns.barplot(
                x=top_10.values,
                y=top_10.index,
                hue=top_10.index,  # <--- NEW: Map hue to Y variable
                legend=False,  # <--- NEW: Hide redundant legend
                ax=ax,
                palette=colors
            )
            # --------------------------

            ax.set_title(f"Root Cause Analysis: SN {sn} (Why is it an outlier?)", fontweight='bold')
            ax.set_xlabel("Contribution to Anomaly Score")

            # Add value labels
            for i, (val, z) in enumerate(zip(top_10.values, raw_z)):
                direction = "(+)" if z > 0 else "(-)"
                ax.text(val, i, f" {direction} {abs(z):.1f}Ïƒ", va='center', fontsize=8)

            self.contrib_fig.tight_layout()
            self.contrib_canvas.draw()

        except Exception as e:
            print(f"Contribution Logic Error: {e}")

    def generate_full_pdf_report(self):
        """Generates a comprehensive, audit-ready PDF Qualification Report."""
        if not self.data_dict:
            messagebox.showinfo("Error", "No data loaded to report.")
            return

        path = filedialog.asksaveasfilename(
            defaultextension=".pdf",
            filetypes=[("PDF Documents", "*.pdf")],
            title="Save Qualification Report"
        )
        if not path:
            return

        # Prepare Shared Data
        unique_sn = set()
        for df in self.data_dict.values():
            if "SN" in df.columns: unique_sn.update(df["SN"].dropna().astype(str).tolist())

        total_units = len(unique_sn)
        unique_fails = set(f['SN'] for f in self.failures)
        fail_count = len(unique_fails)
        yield_pct = ((total_units - fail_count) / total_units * 100) if total_units else 0.0

        try:
            with PdfPages(path) as pdf:

                # =================================================
                # PAGE 1: EXECUTIVE DASHBOARD
                # =================================================
                fig = plt.Figure(figsize=(8.5, 11), dpi=100)

                # Title Block
                ax_title = fig.add_axes([0.1, 0.85, 0.8, 0.1])
                ax_title.axis('off')
                ax_title.text(0.5, 0.7, "Microcircuit Qualification Report", ha='center', fontsize=24,
                              fontweight='bold', color='#2C3E50')
                ax_title.text(0.5, 0.3, f"Date: {datetime.datetime.now().strftime('%Y-%m-%d')}", ha='center',
                              fontsize=12, color='gray')

                # KPI Box (Yield & Counts)
                ax_kpi = fig.add_axes([0.1, 0.65, 0.8, 0.15])
                ax_kpi.axis('off')
                ax_kpi.text(0.15, 0.8, f"TOTAL UNITS: {total_units}", fontsize=12, fontweight='bold')
                ax_kpi.text(0.55, 0.8, f"TOTAL LOTS/STEPS: {len(self.data_dict)}", fontsize=12, fontweight='bold')

                # Dynamic Color for Yield
                yield_color = 'green' if yield_pct > 95 else 'orange' if yield_pct > 90 else 'red'
                ax_kpi.text(0.15, 0.4, f"PASS YIELD: {yield_pct:.1f}%", fontsize=16, fontweight='bold',
                            color=yield_color)
                ax_kpi.text(0.55, 0.4, f"FAILING UNITS: {fail_count}", fontsize=16, fontweight='bold', color='crimson')

                # "High Risk" Warning (Lowest Cpk Params)
                # Find worst 5 Cpks across all sheets
                worst_cpks = []
                for s, df in self.data_dict.items():
                    for c in df.columns:
                        if c == "SN": continue
                        v = df[c].dropna()
                        l, u = self.limits_dict[s].get(c, (np.nan, np.nan))
                        val = self._calc_robust_cpk(v, l, u)
                        if not pd.isna(val) and val < 1.67:
                            worst_cpks.append((val, c, s))

                worst_cpks.sort(key=lambda x: x[0])

                ax_risk = fig.add_axes([0.1, 0.35, 0.8, 0.25])
                ax_risk.axis('off')
                ax_risk.text(0.0, 1.0, "CRITICAL PROCESS RISKS (Lowest Cpk):", fontsize=12, fontweight='bold',
                             color='#E74C3C')

                if worst_cpks:
                    y_pos = 0.85
                    for val, param, sheet in worst_cpks[:8]:
                        cpk_color = 'red' if val < 1.33 else 'orange'
                        ax_risk.text(0.05, y_pos, f"{val:.2f} Cpk  |  {sheet}: {param}", fontsize=10, color=cpk_color,
                                     family='monospace')
                        y_pos -= 0.1
                else:
                    ax_risk.text(0.05, 0.8, "No parameters below 1.67 Cpk detected.", fontsize=10, color='green')

                # Footer
                ax_foot = fig.add_axes([0.1, 0.05, 0.8, 0.05])
                ax_foot.axis('off')
                ax_foot.text(0.5, 0.5, "Page 1 - Executive Summary", ha='center', fontsize=8, color='gray')

                pdf.savefig(fig)

                # =================================================
                # PAGE 2: FAILURE DETAILS & PARETO
                # =================================================
                if self.failures:
                    fig = plt.Figure(figsize=(8.5, 11))

                    # 1. Failure Pareto (Bar Chart)
                    ax_pareto = fig.add_subplot(211)
                    fail_df = pd.DataFrame(self.failures)

                    # Count failures by Parameter
                    counts = fail_df["Parameter"].value_counts().head(10)
                    sns.barplot(x=counts.values, y=counts.index, ax=ax_pareto, palette="Reds_r")
                    ax_pareto.set_title("Top 10 Failing Parameters (Pareto)", fontweight='bold')
                    ax_pareto.set_xlabel("Count of Failures")

                    # 2. Detailed Table (First 25)
                    ax_table = fig.add_subplot(212)
                    ax_table.axis('off')

                    # Clean table data
                    disp_df = fail_df[["Sheet", "SN", "Parameter", "Value", "Type"]].head(25)
                    cols = disp_df.columns
                    cell_text = []
                    for row in disp_df.itertuples(index=False):
                        cell_text.append([str(x) for x in row])

                    table = ax_table.table(cellText=cell_text, colLabels=cols, loc='upper center', cellLoc='left')
                    table.auto_set_font_size(False)
                    table.set_fontsize(8)
                    table.scale(1, 1.2)
                    ax_table.set_title("Failure Log (First 25 Events)", fontweight='bold', y=1.0)

                    fig.tight_layout(rect=[0, 0.05, 1, 0.95])
                    pdf.savefig(fig)

                # =================================================
                # PAGE 3: DRIFT ANALYSIS (Automated Stability Check)
                # =================================================
                # Find the parameter with the highest mean drift
                if self.measure_long is not None:
                    # Filter for numeric params only
                    # We need at least 2 sheets
                    sheets = list(self.data_dict.keys())
                    if len(sheets) >= 2:
                        best_drift_param = None
                        max_drift_mag = -1.0

                        # Scan a subset of params to save time (or all)
                        # Let's check unique base params
                        base_params = list(set([self._get_base(p) for p in self.measure_long["Parameter"].unique()]))

                        for p in base_params[:50]:  # Limit scan to first 50 to avoid freeze
                            # Reuse Mode 5 logic essentially
                            sub = self.measure_long[
                                self.measure_long["Parameter"].apply(lambda x: self._get_base(x) == p)]
                            piv = sub.pivot_table(index="SN", columns="Sheet", values="Value")
                            if piv.shape[1] > 1:
                                drift = (piv.iloc[:, -1] - piv.iloc[:, 0]).abs().mean()
                                # Normalize by value magnitude to find relative drift?
                                # Or just raw. Let's do raw for now.
                                if drift > max_drift_mag:
                                    max_drift_mag = drift
                                    best_drift_param = p

                        if best_drift_param:
                            fig = plt.Figure(figsize=(8.5, 11))
                            ax = fig.add_subplot(111)

                            # Re-run Shift Scatter logic for this param
                            # (Simplified version of Mode 5)
                            sub = self.measure_long[
                                self.measure_long["Parameter"].apply(lambda x: self._get_base(x) == best_drift_param)]
                            piv = sub.pivot_table(index="SN", columns="Sheet", values="Value")
                            valid_sheets = [s for s in sheets if s in piv.columns]

                            x = piv[valid_sheets[0]]
                            y = piv[valid_sheets[-1]]

                            min_v, max_v = min(x.min(), y.min()), max(x.max(), y.max())
                            span = (max_v - min_v) * 0.1

                            ax.plot([min_v - span, max_v + span], [min_v - span, max_v + span], 'k--', alpha=0.3,
                                    label="Ideal Stability")
                            drift_vals = y - x
                            colors = ['red' if d > 0 else 'blue' for d in drift_vals]
                            ax.scatter(x, y, c=colors, alpha=0.6, edgecolors='black')

                            ax.set_title(
                                f"Worst Case Drift Analysis: {best_drift_param}\n({valid_sheets[-1]} vs {valid_sheets[0]})",
                                fontweight='bold', fontsize=14)
                            ax.set_xlabel(f"Initial Value ({valid_sheets[0]})")
                            ax.set_ylabel(f"Final Value ({valid_sheets[-1]})")
                            ax.grid(True, alpha=0.3)

                            # Add text stats
                            stats_txt = f"Mean Drift: {drift_vals.mean():.4f}\nMax Drift: {drift_vals.abs().max():.4f}"
                            ax.text(0.05, 0.95, stats_txt, transform=ax.transAxes,
                                    bbox=dict(facecolor='white', alpha=0.8))

                            pdf.savefig(fig)

                # =================================================
                # PAGE 4: MULTIVARIATE RISK & ROOT CAUSE
                # =================================================
                # 1. Run Assessment
                from sklearn.decomposition import PCA
                from sklearn.preprocessing import StandardScaler
                import scipy.stats as stats
                from scipy.spatial import distance

                df_wide = self.measure_long.pivot_table(index="SN", columns="Parameter", values="Value", aggfunc='mean')
                df_clean = df_wide.dropna(axis=1, how='any')
                df_clean = df_clean.loc[:, df_clean.std() > 1e-9]

                if df_clean.shape[1] > 2:
                    fig = plt.Figure(figsize=(8.5, 11))

                    # -- Plot 1: The Maverick Plot --
                    ax_mv = fig.add_subplot(211)

                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(df_clean)
                    pca = PCA(n_components=0.99)
                    X_pca = pca.fit_transform(X_scaled)

                    cov = np.cov(X_pca.T)
                    inv_cov = np.linalg.inv(cov)
                    mean_pca = X_pca.mean(axis=0)
                    centered = X_pca - mean_pca
                    m_dists = [distance.mahalanobis(c, np.zeros_like(c), inv_cov) for c in centered]

                    dof = X_pca.shape[1]
                    threshold = np.sqrt(stats.chi2.ppf(0.999, dof))

                    colors = ['red' if x > threshold else 'steelblue' for x in m_dists]
                    ax_mv.scatter(range(len(m_dists)), m_dists, c=colors, alpha=0.7)
                    ax_mv.axhline(threshold, color='red', ls='--', label=f"Limit: {threshold:.1f}")
                    ax_mv.set_title("Multivariate Outlier Detection (Mavericks)", fontweight='bold')
                    ax_mv.set_ylabel("Mahalanobis Distance")

                    # -- Plot 2: Auto-Root Cause for #1 Outlier --
                    # Find max distance index
                    max_idx = np.argmax(m_dists)
                    max_dist = m_dists[max_idx]

                    if max_dist > threshold:
                        ax_rc = fig.add_subplot(212)

                        # Run decomposition logic manually
                        sn_idx = max_idx
                        pca_scores = X_pca[sn_idx]
                        loadings = pca.components_
                        weighted = pca_scores.reshape(-1, 1) * loadings
                        impact = np.abs(weighted).sum(axis=0)

                        feats = df_clean.columns
                        impact_s = pd.Series(impact, index=feats).sort_values(ascending=False).head(10)

                        # Plot Bar
                        sns.barplot(x=impact_s.values, y=impact_s.index, ax=ax_rc, palette="Reds_r")
                        ax_rc.set_title(f"Root Cause Analysis for Top Outlier (SN {df_clean.index[max_idx]})",
                                        fontweight='bold')
                        ax_rc.set_xlabel("Contribution to Anomaly Score")
                    else:
                        ax_rc = fig.add_subplot(212)
                        ax_rc.text(0.5, 0.5, "No Statistical Outliers Detected.\n(Root Cause Analysis Skipped)",
                                   ha='center')
                        ax_rc.axis('off')

                    fig.tight_layout()
                    pdf.savefig(fig)

                # =================================================
                # PAGE 5: PHYSICS CLUSTERING (Dendrogram)
                # =================================================
                import scipy.cluster.hierarchy as sch
                first_sheet = list(self.data_dict.keys())[0]
                df_phys = self.data_dict[first_sheet].select_dtypes(include=[np.number])
                df_phys = df_phys.loc[:, df_phys.std() > 1e-9]

                if df_phys.shape[1] > 1:
                    fig = plt.Figure(figsize=(8.5, 11))
                    ax_d = fig.add_subplot(111)

                    corr = df_phys.corr()
                    dist = 1 - corr.abs()
                    linkage = sch.linkage(sch.distance.squareform(dist), method='ward')

                    sch.dendrogram(linkage, labels=df_phys.columns, ax=ax_d, leaf_rotation=90, leaf_font_size=8)
                    ax_d.set_title(f"Device Physics Map (Dendrogram)\nSheet: {first_sheet}", fontweight='bold')
                    ax_d.set_ylabel("Dissimilarity (Height)")

                    fig.tight_layout()
                    pdf.savefig(fig)

            messagebox.showinfo("Success", f"Professional Report Saved:\n{path}")

        except Exception as e:
            messagebox.showerror("Report Error", f"Failed to generate PDF:\n{str(e)}")

    def calculate_assessment(self):
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler
        import scipy.stats as stats

        try:
            # 1. Gather Data & Clean
            if self.measure_long is None: return

            # Pivot to Wide Format
            df_wide = self.measure_long.pivot_table(index="SN", columns="Parameter", values="Value", aggfunc='mean')

            # Drop columns with NaNs or Zero Variance
            df_clean = df_wide.dropna(axis=1, how='any')
            df_clean = df_clean.loc[:, df_clean.std() > 1e-9]

            if df_clean.shape[1] < 2:
                self.txt_assess.delete("1.0", tk.END)
                self.txt_assess.insert(tk.END, "Insufficient data for multivariate analysis.")
                return

            # 2. PCA (Dimensionality Reduction) - The Robustness Fix
            # We scale the data first (Critical for PCA/Mahalanobis)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(df_clean)

            # Keep components explaining 99% of variance (Usually drops 190 vars down to ~20)
            pca = PCA(n_components=0.99)
            X_pca = pca.fit_transform(X_scaled)

            # 3. Calculate Mahalanobis Distance on the PCA components
            # Since PCA components are uncorrelated and unit variance (if whitened, but standard is close),
            # Mahalanobis is simply the Euclidean distance from origin in PCA space.

            # More robustly: Calculate covariance of the PCA scores
            cov = np.cov(X_pca.T)
            inv_cov = np.linalg.inv(cov)

            mean_pca = X_pca.mean(axis=0)
            centered_pca = X_pca - mean_pca

            m_dists = []
            for i in range(len(centered_pca)):
                x = centered_pca[i]
                d = distance.mahalanobis(x, np.zeros_like(x), inv_cov)
                m_dists.append(d)

            df_clean["Mahalanobis"] = m_dists

            # 4. Univariate Score (for X-Axis context)
            z_scores = (df_clean - df_clean.mean()) / df_clean.std()
            z_cols = [c for c in z_scores.columns if c != "Mahalanobis"]
            df_clean["UnivarScore"] = z_scores[z_cols].abs().sum(axis=1)

            # 5. Threshold Calculation (The Bug Fix)
            # Degrees of freedom = Number of PCA components kept (not original vars)
            dof = X_pca.shape[1]

            # CRITICAL FIX: Take SQRT of Chi2 to match the linear Distance scale
            limit_sq = stats.chi2.ppf(0.999, dof)
            threshold = np.sqrt(limit_sq)

            outliers = df_clean[df_clean["Mahalanobis"] > threshold]

            # 6. Update UI Text
            txt = "MULTIVARIATE LOT ASSESSMENT (PCA-Enhanced)\n" + "=" * 40 + "\n"
            txt += f"Original Variables: {df_wide.shape[1]}\n"
            txt += f"PCA Components (99% Var): {dof}\n"
            txt += f"Total Units: {len(df_clean)}\n"
            txt += f"Multivariate Outliers (p<0.001): {len(outliers)}\n"

            if not outliers.empty:
                txt += "\nTOP MAVERICKS:\n"
                for sn, row in outliers.sort_values("Mahalanobis", ascending=False).head(5).iterrows():
                    txt += f"  SN {sn}: Dist={row['Mahalanobis']:.2f} (Limit={threshold:.1f})\n"
            else:
                txt += "\nNo statistical outliers detected.\n"

            self.txt_assess.delete("1.0", tk.END)
            self.txt_assess.insert(tk.END, txt)

            # 7. Update Table
            self.tree_rank.delete(*self.tree_rank.get_children())
            ranked = df_clean.sort_values("Mahalanobis", ascending=False)
            fail_counts = pd.DataFrame(self.failures)["SN"].value_counts() if self.failures else pd.Series(dtype=int)

            for i, (sn, row) in enumerate(ranked.iterrows(), start=1):
                self.tree_rank.insert("", "end", values=(
                    i, sn, f"{row['Mahalanobis']:.2f}", f"{row['UnivarScore']:.2f}", fail_counts.get(sn, 0)
                ))

                # STORE DATA FOR CLICK HANDLER
                self.pca_model = pca
                self.pca_data_clean = df_clean  # Original values (for lookup)
                self.pca_transformed = X_pca  # PCA scores
                self.pca_scaler = scaler  # To un-scale if needed
                self.pca_inv_cov = inv_cov  # For math

            # 8. Plotting
            self.assess_fig.clf()
            ax = self.assess_fig.add_subplot(111)

            # Dynamic Colors: Red for outliers, Blue for normal
            colors = ['red' if x > threshold else 'steelblue' for x in df_clean["Mahalanobis"]]

            ax.scatter(df_clean["UnivarScore"], df_clean["Mahalanobis"], alpha=0.6, edgecolors='black', c=colors)

            # Plot the Limit Line
            ax.axhline(threshold, color='red', linestyle='--', label=f"Limit: {threshold:.1f}")

            # Auto-Scale Y-Axis to make data visible (fix "Lumping")
            max_y = max(threshold * 1.2, df_clean["Mahalanobis"].max() * 1.1)
            ax.set_ylim(0, max_y)

            ax.set_title(f"Outlier Detection (PCA: {dof} Dim)", fontweight='bold')
            ax.set_xlabel("Univariate Score")
            ax.set_ylabel("Mahalanobis Distance")
            ax.legend()
            ax.grid(True, alpha=0.3)

            # Annotate outliers
            for sn, row in outliers.iterrows():
                ax.text(row["UnivarScore"], row["Mahalanobis"], str(sn), fontsize=8, color='darkred', fontweight='bold')

            self.assess_canvas.draw()

        except Exception as e:
            messagebox.showerror("Assessment Error", f"Analysis failed:\n{str(e)}")

    # ----------------- Report / export -----------------
    def generate_screen_report(self):
        for w in self.report_frame.winfo_children():
            w.destroy()

        ttk.Label(self.report_frame, text="Screen Report", font=("Arial", 14, "bold")).pack(anchor="w", pady=5)

        summary = tk.Text(self.report_frame, height=8, font=("Consolas", 10))
        summary.pack(fill=tk.X, padx=5, pady=5)

        total_sn = set()
        for df in self.data_dict.values():
            if "SN" in df.columns:
                total_sn.update(df["SN"].dropna().astype(str).tolist())

        summary.insert(
            tk.END,
            f"Total lots/devices (unique SN): {len(total_sn)}\n"
            f"Total failures: {len(self.failures)}\n"
            f"Total parameters with limits: {len(self.param_map)}\n",
        )

        cols = ("Parameter", "Count", "Mean", "Std", "Min", "Max")
        tree_stats = ttk.Treeview(self.report_frame, columns=cols, show="headings", height=15)
        for c in cols:
            tree_stats.heading(c, text=c)
            tree_stats.column(c, anchor="w", width=120)
        tree_stats.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        stats = {}
        for s, df in self.data_dict.items():
            for c in df.columns:
                if c == "SN":
                    continue
                base = self._get_base(c)
                vals = df[c].dropna()
                if vals.empty:
                    continue
                if base not in stats:
                    stats[base] = []
                stats[base].append(vals)

        for p, vlist in stats.items():
            allv = pd.concat(vlist)
            tree_stats.insert(
                "",
                "end",
                values=(
                    p,
                    int(allv.count()),
                    f"{allv.mean():.3f}",
                    f"{allv.std():.3f}",
                    f"{allv.min():.3f}",
                    f"{allv.max():.3f}",
                ),
            )

    def export_failures_csv(self):
        if not self.failures:
            messagebox.showinfo("Info", "No failures to export.")
            return
        path = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV", "*.csv")])
        if not path:
            return
        pd.DataFrame(self.failures).to_csv(path, index=False)
        messagebox.showinfo("Export", f"Failures exported to {path}")

    def export_assessment_csv(self):
        if not self.data_dict:
            messagebox.showinfo("Info", "No data loaded.")
            return
        all_z = []
        for s, df in self.data_dict.items():
            cols = [c for c in df.columns if c != "SN"]
            num = df[cols].select_dtypes(include=[np.number])
            if "SN" in df.columns and not num.empty:
                num = num.copy()
                num.index = df["SN"].astype(str)
                z = (num - num.mean()) / num.std()
                all_z.append(z)
        if not all_z:
            messagebox.showinfo("Info", "No numeric data for assessment.")
            return
        full_z = pd.concat(all_z, axis=1).fillna(0)
        abs_z = full_z.abs()
        scores = abs_z.sum(axis=1)
        max_z = abs_z.max(axis=1)
        cnt_z3 = (abs_z > 3).sum(axis=1)
        cnt_z6 = (abs_z > 6).sum(axis=1)

        rank = pd.DataFrame({
            "SN": scores.index,
            "DeviationScore": scores.values,
            "MaxZ": max_z.values,
            "Count|Z|>3": cnt_z3.values,
            "Count|Z|>6": cnt_z6.values,
        }).sort_values("DeviationScore")

        path = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV", "*.csv")])
        if not path:
            return
        rank.to_csv(path, index=False)
        messagebox.showinfo("Export", f"Assessment exported to {path}")

    # ----------------- Helpers -----------------
    def _process_headers(self, raw):
        clean = [str(h).strip() for h in raw if str(h).lower() != "nan"]
        if len(clean) > 5:
            b = len(clean) // 3
            if b > 0:
                s1, s2 = set(clean[:b]), set(clean[b : 2 * b])
                if b > 0 and len(s1 & s2) / b > 0.7:
                    self.is_tritemp = True
                    new_h = []
                    for i, h in enumerate(raw):
                        s = str(h).strip()
                        if i <= self.var_sn_col.get() + 1:
                            new_h.append(s)
                        else:
                            adj = i - (self.var_sn_col.get() + 2)
                            sub = max(1, (len(raw) - 2) // 3)
                            if adj < sub:
                                t = "(Room)"
                            elif adj < 2 * sub:
                                t = "(Cold)"
                            else:
                                t = "(Hot)"
                            new_h.append(f"{s} {t}")
                    return self._unique(new_h)
        return self._unique(raw)

    def _unique(self, headers):
        seen, out = {}, []
        for x in headers:
            s = str(x).strip()
            seen[s] = seen.get(s, -1) + 1
            out.append(f"{s}.{seen[s]}" if seen[s] > 0 else s)
        return out

    def _get_base(self, p):
        for t in ["(Room)", "(Cold)", "(Hot)"]:
            p = p.replace(t, "").strip()
        return p

    def _calc_cpk(self, v, l, u):
        if v.empty or v.std() == 0:
            return np.nan
        m, s = v.mean(), v.std()
        cpu = (u - m) / (3 * s) if not pd.isna(u) else np.nan
        cpl = (m - l) / (3 * s) if not pd.isna(l) else np.nan
        if pd.isna(cpu) and pd.isna(cpl):
            return np.nan
        if pd.isna(cpu):
            return cpl
        if pd.isna(cpl):
            return cpu
        return min(cpu, cpl)

    def _calc_robust_cpk(self, v, l, u):
        # 1. Safety Check: Not enough data
        if v.empty or len(v) < 5:
            return np.nan

        # 2. Check for Zero Variance (Constant data)
        # If the standard deviation is effectively 0, Cpk is technically infinite
        # (if within spec) or 0 (if out of spec). We return NaN to avoid plot errors.
        if v.std() < 1e-9:
            return np.nan

        # 3. Normality Test (Shapiro-Wilk)
        # If p > 0.05, we assume Normal distribution.
        stat, p_value = stats.shapiro(v)
        is_normal = p_value > 0.05

        cpu = np.inf
        cpl = np.inf

        if is_normal:
            # --- Parametric Calculation (Mean/Std) ---
            m, s = v.mean(), v.std()

            # Calculate Upper/Lower Cpk only if limits exist
            if not pd.isna(u):
                cpu = (u - m) / (3 * s)
            if not pd.isna(l):
                cpl = (m - l) / (3 * s)

        else:
            # --- Non-Parametric Calculation (Percentiles) ---
            median = v.median()
            # 99.865th percentile ~= +3 sigma
            p99_865 = np.percentile(v, 99.865)
            # 0.135th percentile ~= -3 sigma
            p0_135 = np.percentile(v, 0.135)

            # Safety: Prevent division by zero if percentiles equal median (flat data)
            upper_spread = max(1e-9, p99_865 - median)
            lower_spread = max(1e-9, median - p0_135)

            if not pd.isna(u):
                cpu = (u - median) / upper_spread
            if not pd.isna(l):
                cpl = (median - l) / lower_spread

        # 4. Final Result Handling
        result = min(cpu, cpl)

        # If both limits were NaN, result is still inf. Return NaN instead.
        if result == np.inf:
            return np.nan

        return result

    def _sort_cpk_tree(self, col, reverse):
        data = [(self.tree_cpk.set(k, col), k) for k in self.tree_cpk.get_children("")]
        if col in ("Cpk", "Mean"):
            data.sort(key=lambda t: float(t[0]), reverse=reverse)
        else:
            data.sort(key=lambda t: t[0], reverse=reverse)
        for idx, (_, k) in enumerate(data):
            self.tree_cpk.move(k, "", idx)
        self.tree_cpk.heading(col, command=lambda: self._sort_cpk_tree(col, not reverse))


if __name__ == "__main__":
    root = tk.Tk()
    app = QualAnalyzerV20(root)
    root.mainloop()
